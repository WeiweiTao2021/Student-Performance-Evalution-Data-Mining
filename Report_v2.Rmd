---
title: "Secondary School Student Performance Analysis"
author: "AMS 572 Final Project"
date: "Dec 1, 2021"
output:
  pdf_document:
    fig_caption: yes
  html_document:
    df_print: paged
---
```{r packages, echo=FALSE,include=FALSE}
# clear variable environment
rm(list=ls(all=TRUE))

# set random seeds
set.seed(572)

# load all packages
list.of.packages <- c("tableone","dplyr","readxl", "rtf", "caret","psych","ggpubr","gtsummary", "PerformanceAnalytics", 'gridExtra', 'grid', 'GGally', 'ggfortify',"lme4", "Hmisc",
              "kableExtra", "plyr","ggplot2","pscl","corrplot",'DescTools','pastecs',
              "boot", "stringi","multcomp","stringr", "tidyr",
              "randomForest", "caret", 'leaps', 'knitr', "mice")

# check whether required packages were installed, if not install it.
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

lapply(list.of.packages, library, character.only=TRUE)

# set directory to current project directory
knitr::opts_knit$set(root.dir = '/tmp')

# function for table presentation
kable1 <- function(x, caption, font_size) {
  capture.output(x <- print(x, printToggle=FALSE,
                            noSpaces=FALSE, showAllLevels=TRUE,
                            test=TRUE, varLabels=TRUE))
  knitr::kable(x, caption = caption) %>%
    kable_styling(bootstrap_options = c("condensed", "hover"), font_size = font_size)
}

proper = function(x) paste0(toupper(substr(x, 1, 1)), tolower(substring(x, 2)))

# read source document
df <- read.csv(file = 'student-mat.csv', sep = ';')
df$id <- c(1:nrow(df))
```

## Introduction
For this specific study, high schoolers during the 2005 - 2006 school year from two public schools in  the Alentejo region of Portugal were questioned in an effort to gain insight on variables that can predict a student's success in school. What inspired this collection of data and gives it value to analyze is the issue Portugal has with graduation rates among its population. As of 2012, only 28% of Portuguese adults over the age of 30 received a high school diploma, severely lagging behind other Western European countries. With a mix of high dropout rates and failures in high school, Portugal is left to deal with these implications. Education paves the way to improve individual lives by opening possibilities otherwise left unknown, as well as improving society as a whole since better education improves the economy and almost all other factors. That is why it is imperative to invest properly in education and improve the retention rate of schools. Which can be done looking at collected data. 

In this data set, 395 high school students students from two different schools in math classes were given a questionnaire that had 37 questions to collect real world data. To accompany these questionnaires, the accompanying students' school reports were also included. This contained data such as the G1 and G2 results which were the grades at the end of 2 periods and then G3 which was the final evaluation grade. For the grades, the school system used a system of 20 points where 20/20 represented a 100% for the grade. The purpose for collecting data sets like this is to be able to see the relations between different variables in a students performance and lives that will impact their success rate in school. For places like Portugal in particular this is key if a better graduation rate is wanted. 

The dependent variables that will be used for this study would be the G1, G2, and G3 results. These are the grades throughout the school year at different periods with G3 being the final end of year grade. Therefore the relation between the other variables and these final grades is what is investigated.

Demographic information on the students were collected including: which school they attend, student sex, age, living location (rural or urban), availability of internet at home, reason for choosing their current school, travel time, and current health status. These simple questions can affect a students performance due to maturity or at home tasks related to societal gender roles and expectations.

Furthermore, family information was asked about: family size, if the parents live together or separate, each parents education level along with current occupation, the students legal guardian, and the quality of relations with their family. Family is a known factor in students' success rates as the background plays a huge role in the opportunities a child has. Lastly, information on the student outside and during school was collected on: study time per week, number of past class failures, if receiving extra education support both in and out of school, participating in extracurricular activities, attended nursery school, in a romantic relationship, amount of freetime after school, time spent going out with friends, workday and weekend alcohol consumption, and number of school absences. 

For the dataset of interest and to be analyzed in this study, there are a total of 395 observations of students from both schools that participated in the math course. Between the two types of data collection methods (student grade reports and questionnaires), there are a total of 30 independent variables in addition to G1, G2 and G3. Three are continuous variables which are: students age, number of past failures, and the number of class absences. All the remaining variables are categorical being either binary yes or no type, or categories ranked in a 1-5 system. 


## Materials and Methods
In any study, missing data can result in significant change in the results and therefore meaning of the analysis. That is why great lengths are taken to avoid large vacancies and ensure the sample collected is representative of the total population and is distributed correctly. In this analysis, two forms of missing data are investigated to observe what effects they will have on the dataset of students' performance in a math class and to see if the relationship between the different variables tells a different story. 

First, missing data not at random (MNAR) exists in the dataset. With this particular dataset, it is observed that some students received zeros for either G2 or G3 or both. Those students tend to have a lower G1 score as comparing to other students. This indicates that some students did in fact drop out of the class and therefore is left with missing data. This scenario is used to analyze the MNAR. By doing this, the relationship between the final grade (G3) and the other variables could be affected due to survival bias and possible better conditions to foster the proficient grades.

Secondly, Missing at Completely Random (MCAR) data points will also be simulated. With this scenario, there is a given proportion of data values that are not present in the set. This is not from any given reason and is randomly distributed throughout a single variable or the entire study. For this particular study, the data set is run through code that randomly imputes the missing data in an identical sample of the original unperturbed one. Then the number of missing values is accounted for to allow for an easier analysis. It is noted that almost all students (the 395 observations), will receive either 0 or 1 missing data in one of the variables they were questioned on. 



For the first hypothesis, the equivalency of the G1 and G3 as well as G2 and G3 grades were to be tested. By doing so, we would be able to see if there is a change in the overall average students performance and class distribution from the first period (G1) to the final (G3). It is expected that the two grades are in fact not significantly different and that the performance of the student by G1 will result in the same pass/fails as the final G3 reports. This can serve as a tool to make early predictions within the classroom and to adapt the focus on certain students or teaching styles early on in the school year. 
For this analysis, a paired t-test is selected as the best fit for this hypothesis. 


In addition to investigating the relationship between G1 and G3 average grades, the second hypothesis will look into what factors in a students academic and personal life will affect their final G3 grade in the math class. These results will be collected using the method of multiple linear regression. While a student’s final performance may appear to be solely dependent on their other grades and perhaps even by chance, many factors can be at play that dictate how a student will end up. Life is never as simple as moving from one grade to another and in fact has many variables in circulation. That is where this study focused the majority of its time. With the combination of the student reports and personal questionnaires, its goal is to highlight any possible connections. This will allow the school and larger entities to better predict how a student body will perform and where to focus on improvements. By conducting linear regressions on the variables connected to the G3 grade, the ones showing the highest correlation will be investigated further to compute a final equation that can be used to propose an accurate determination of a students final grade with the input to a select few variables.

Finally, the finally hypothesis is to analyze what factors will affect an improvement in students' score the most. Specifically, what is the main driving force to lead to a decrement in performance in final grade as comparing to the first examination. We used logistic regression model to examine the data and results will be presented in the last section.

```{r, echo=FALSE,include=FALSE}
# data preprocessing replace numbers with actual meaning
df$address = ifelse(df$address=='U',"Urban", "Rural")
df$Pstatus = ifelse(df$Pstatus=='T',"LiveTogether", "Apart")
df$Medu = ifelse(df$Medu==0, "None", 
                 ifelse(df$Medu==1, "<=4th grade", 
                        ifelse(df$Medu==2, "5th to 9th grade",
                               ifelse(df$Medu==3, "secondary education", "higher education"))))

df$Fedu = ifelse(df$Fedu==0, "None", 
                 ifelse(df$Fedu==1, "<=4th grade", 
                        ifelse(df$Fedu==2, "5th to 9th grade",
                               ifelse(df$Fedu==3, "secondary education", "higher education"))))

df$traveltime = ifelse(df$traveltime==1, "<15 min", 
                 ifelse(df$traveltime==2, "15-30 min", 
                        ifelse(df$traveltime==3, "30-60 min", ">1 hour")))

df$studytime = ifelse(df$studytime==1, "<2 hours", 
                 ifelse(df$studytime==2, "2-5 hours", 
                        ifelse(df$studytime==3, "5-10 hours", ">10 hours")))
```

## Summary Statistics
We first generated summary statistics for all variables. The goal is to describe the main features of numerical and categorical information with simple summaries. For categorical variables, we calculated the frequency and proportions of each category. The P-value was calculated for each categorical variable using one-way ANOVA test to compare the means of G3 at each category. Note that ANOVA may not be suitable for some variables where the number of observations is less than 30. P-values presented provide a baseline reference on relationships between each variables and the final grade.

For continuous variables, we presented the minimum, maximum, mean, and standard deviations of each variable.
```{r,echo=FALSE,include=TRUE}
## generate summary statistics
charcol <- colnames(df)[grepl('factor|logical|character',sapply(df,class))]
temp <- colnames(df)[!grepl('factor|logical|character',sapply(df,class))]
numcol <- c(temp[1:12])

nout <- data.frame()
for(col in numcol){
  mci <- mean_ci(df[,col])
  a <- data.frame(col, min(df[,col]), max(df[,col]), round(mean(df[,col]),2), round(sd(df[,col]),2))
  colnames(a) <- c('Variables', 'Min', 'Max', 'Mean', 'SD')

  nout <- rbind(nout, a)
}
nout$Variables <- proper(nout$Variables)

cout <- data.frame()
for(col in charcol){
  a <- Freq(table(df[col]))
  a$level <- paste0(col,'_', a$level)
  
  b <- aggregate(df$G3, by=df[col], FUN=mean)
  a <- cbind(a, b[2])
  names(a)[names(a) == 'x'] <- "G3 (Mean)"
  
  a$`G3 (Mean)` <- round(a$`G3 (Mean)` ,2)
  a$perc <- round(a$perc ,2)
  
  df1 <- df[c("G3", col)]
  names(df1)[names(df1) == col] <- "x"
  res <- summary(aov(G3~x,data=df1))
  a$p_value[1] <- round(res[[1]]$`Pr(>F)`[1], 3)
  a$p_value[2:nrow(a)] <- NA

  cout <- rbind(cout, a)
}
cout$cumfreq <- NULL
cout$cumperc <- NULL
colnames(cout) <- c('Variables', 'Frequency', 'Percentage', 'G3 (Mean)', 
                    'P-value (G3)')
cout$Variables <- proper(cout$Variables)

kable1(cout, 'Frequency Distribution of Categorical Variables', font_size = 8)

kable1(nout, 'Summary Statistics of Numerical Variables', font_size = 11)

df <- df %>%
  mutate_at(charcol, as.factor)
```
Table 1 shows the summary statistics of all categorical variables. Most students in our dataset are from an urban setting(78%). 71% students come from a family with more than 3 people. 33% of the students' mothers have received education higher than 9th grade and 24% of students' fathers received a higher education. 50% of students study 2-5 hours every week while only 16% students study more than 5 hours per week. 13% students take extra educational school support and  61% students have extra family educational support. 95% students are planning to apply for a higher degree beyond high school and those who do not plan to go towards a higher degree has substantially lower average final grade than other students.

## Deal with Missing Values
In these dataset, there are missing values in father's education and mother's education, which are labeled as 'None' in the original data provide. Students tend to have a higher final grade for those who are missing parents' education and there are only 3 students missing those two columns. We will leave it as what it is as a 'None' category.

There are also missing values in G2 and G3 and it is suspected that the missing data are not at random. Here, the missing entries are considered 0's for the students who did not show for exam 2, exam 3, or both. We plot histograms in Figure 1 and Figure 2 to show the distribution of grades for the first exam (G1), where students received a zero for the second exam (G2) and final exam (G3). 

When missing values are entered in a data set, it could make the data biased. One benefit to using imputation is that we could hypothesize the data entry for any missing entries. Two kinds of missing values that we will use are MCAR and MNAR. For better organization, we transform the categorical data to have binary values. The imputing data functions will not work with categorical data.

### MNAR (Missing Not at Random)
When a data set has MNAR, there is an external factor that is affecting the missing entries. Since our data set involves a third grade that could be influenced by the student's progress in the course, it is possible that some students withdrew from the course due to lack of performance in the class.

Since most of the grades are mostly in the lower half of the distribution (less than mean of G1: 10.91	), this supports our assumption that the data set includes MNAR. In addition, it is important to note that when there is a 0 in the G2 column, there is also a 0 in the G3 column. This dismisses any thought that the student may have missed G2 but is still enrolled in the class.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=5,fig.height=3.5, fig.align='center', fig.cap = "Distribution of G1 grade for students whose G2 equals 0"}
missing <- df$G1[df$G2==0]
hist(missing, breaks = 3, xlim=c(0,20), main = "",
     xlab = "G1 grade", ylab = "Number of Students with G2=0", col = "blue")
```

```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=4, fig.align='center', fig.cap = "Distribution of G1 and G2 for students whose G3 equals 0"}
scraped <- df$G1[df$G3==0]
par(mfrow=c(1,2))
hist(scraped, breaks = 3, xlim=c(0,20),  main = "",
     xlab = "G1 grade", ylab = "Number of Students with G3=0", col = "blue")

scraped <- df$G2[df$G3==0]
hist(scraped, breaks = 3, xlim=c(0,20),  main = "",
     xlab = "G2 grade", ylab = "Number of Students with G3=0", col = "blue")
```

In order to simulate missing exam scores, we used a for loop to enter "NA" for the entries that are "0" within the columns G2 and G3.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth'}
i = 1
file.mnar.copy <- df
for( i in 1:395){
  if (df[i,32] == 0){
    file.mnar.copy[i,32] <- NA
  }
}

j = 1
for(j in 1:395){
  if (df[j,33] == 0){
    file.mnar.copy[j, 33] <- NA
  }
}

```

```{r, echo = FALSE}
column = 1
number.of.NA.values = {}
column.name = colnames(file.mnar.copy)[1:33]

for(column in 0:33){
  number.of.NA.values[column] = sum(is.na(file.mnar.copy[,column]))
}
na_summary <- data.frame(column.name, number.of.NA.values)
print(paste0("There are ",na_summary$number.of.NA.values[31], " missing values in G1"))
print(paste0("There are ",na_summary$number.of.NA.values[32], " missing values in G2"))
print(paste0("There are ",na_summary$number.of.NA.values[33], " missing values in G3"))
print(paste0("There are ",sum(is.na(file.mnar.copy$G2) & is.na(file.mnar.copy$G3)), " both missing G3 and G2"))
```

** Imputing Missing Data **
We used the package MICE (Multivariate Imputation by Chained Equations) to impute the missing data. By default, mice() calculates five (m = 5) imputed data sets. We use the final imputation for our final analysis. According to the summary data, we find that the minimum values for G2 and G3 are all greater than 0 now.
```{r, echo = FALSE}
#md.pattern(file.mnar.copy, plot = TRUE)
imputed <- mice(file.mnar.copy, printFlag = FALSE)
df_MNAR <- complete(imputed)
#imputed$imp$G2
#imputed$imp$G3
#md.pattern(MNAR_complete)
summary(df_MNAR[,c('G1', 'G2', 'G3')])
```

### MCAR (Missing Completely at Random)
When missing values are MCAR, the probability of having a missing value in each category is equal. The MICE (Multivariate Imputation by Chained Equations) package contains the function "ampute" that is used to randomly assign values in our dataset. This is done by specifying that the mechanism is MCAR in one of the parameters.

Similarly, we can use mice package to impute the missing data when it is MCAR. In our case, when there are only a few missing data point (less than 10%), we can directly remove those record with missing values from our analysis. 
```{r, echo = FALSE,include=TRUE}
mads_mcar <- ampute(data = df_MNAR, mech = "MAR")
mcar <- mads_mcar$amp
```

```{r, echo = FALSE, results = 'hide'}
imputed <- mice(mcar, printFlag = FALSE)
MCAR_complete <- complete(imputed)
```



Create data frame containing the number of random missing values for each column. From the below table, the randomization of "NA" entries seems to be completely random.
```{r,echo=FALSE,include=TRUE}
column = 1
number.of.NA.values = {}
column.name = colnames(df)[1:33]
for(column in 0:33){
  number.of.NA.values[column] = sum(is.na(mcar[,column]))
}

missing_summary <- t(data.frame(Features = column.name, Nmissing = number.of.NA.values))
out <- rbind(missing_summary[,c(1:11)], missing_summary[,c(12:22)], missing_summary[,c(23:33)])
kable1(out, 'Summary of missing values for each columns', font_size = 8)
```
Moreover, the number of missing entries per student can also be analyzed. The number of randomly assigned values for each student is almost always either 0 or 1.
```{r,echo=FALSE}
sample = 1
student.number = {1:395}
for(sample in 1:395){
  number.of.NA.values[sample] = sum(is.na(mcar[sample,]))
}
a <- data.frame(student.number, number.of.NA.values)
```

```{r, echo=FALSE,include=FALSE}
names(df_MNAR)[names(df_MNAR) == 'Medu'] <- "Mother Education"
names(df_MNAR)[names(df_MNAR) == 'Mjob'] <- "Mother Job"
names(df_MNAR)[names(df_MNAR) == 'Fedu'] <- "Father Education"
names(df_MNAR)[names(df_MNAR) == 'Fjob'] <- "Father Job"
names(df_MNAR)[names(df_MNAR) == 'famrel'] <- "Family Relationship"
names(df_MNAR)[names(df_MNAR) == 'failures'] <- "Past Failures"
names(df_MNAR)[names(df_MNAR) == 'Walc'] <- "Weekend alcohol"
names(df_MNAR)[names(df_MNAR) == 'Dalc'] <- "Workday alcohol"
```

## Data Visualization
For this study, we are interested in what are the factors that will affect final score. According to Table 1, among all categorical variables, G3 of different levels of following variables have shown statistically significant difference between each other (p-value from ANOVA test < 0.05):
- Mother's education
- Father's education
- Mother's job
- Extra paid classes
- Willingness to take higher education
- In a romantic relationship

To visualize the variability of G3 within each group, we presented boxplots for each of those variables in Figure 1 using data imputed from MNAR missing.The boxplot shows that parents with a higher degree and the students who are willing to pursue a higher degree will tend to have a higher final grades. Students with a romantic relationship have lower final grades than other students.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=12, fig.align='center', fig.cap = "Boxplots for G3 by selected variables."}

g1 <- ggboxplot(df_MNAR, 
          x = "Mother Education", 
          y = "G3", 
          color = "Mother Education", 
          ylab = "Final Grade", 
          xlab = "Mother's Education")+
      rotate_x_text(angle = 45)

g2 <- ggboxplot(df_MNAR, 
          x = "Mother Job", 
          y = "G3", 
          color = "Mother Job", 
          ylab = "Final Grade", 
          xlab = "Mother's Job")+
      rotate_x_text(angle = 45)

g3 <- ggboxplot(df_MNAR, 
          x = "Father Education", 
          y = "G3", 
          color = "Father Education", 
          ylab = "Final Grade", 
          xlab = "Father's Education")+
      rotate_x_text(angle = 45)

g4 <- ggboxplot(df_MNAR, 
          x = "paid", 
          y = "G3", 
          color = "paid", 
          ylab = "Final Grade", 
          xlab = "Extra Paid Classes")

g5 <- ggboxplot(df_MNAR, 
          x = "higher", 
          y = "G3", 
          color = "higher", 
          xlab = "Willingness to take higher education", 
          ylab = "Final Grade") 
  

g6 <- ggboxplot(df_MNAR, 
          x = "romantic", 
          y = "G3", 
          color = "romantic", 
          ylab = "Final Grade", 
          xlab = "With a Romantic Relationship")

grid.arrange(g1, g2, g3,g4, g5, g6, ncol = 2, nrow = 3)
```
Table 2 presents summary statistics of all continuous and ordinal variables. Family relationship, free-time, going out, workday alcohol, weekend alcohol, and health are ordinal variables with 1 representing the minumum and 5 representing the maximum. Here "diff1" is the difference between G3 and G1 ($G3-G1$) while "diff2" equals to $G3-G2$.

According to Table 2, students' age varies from 15 to 22 with 0 to 3 failures. On average, final grades $G3$ is lower than $G1$ and $G2$. $G1, G2, G3$ range from 0 to 20 with a mean around 10. A score of 0 may be due to dropping out of the class or missing the exams. To visualize the distribution of scores at each period, Figure 2 shows the histograms of each grade.  The Shapiro-Wilk normality test shows that the scores deviate significantly from normality. However, it is (a bit strongly stated) a fact that formal normality tests always reject on the huge sample sizes we work with today (reference: https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless). Histograms shows that the grades follow bell-shaped distribution with the average score around 10.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=3.5, fig.align='center', fig.cap = "Distribution of grades at each period."}
shapiro.test(df_MNAR$G3)
shapiro.test(df_MNAR$G2)
shapiro.test(df_MNAR$G1)

p1 <- ggplot(df_MNAR, aes(x = G1, y = ..density..)) +
  geom_histogram(fill = "cornsilk", colour = "grey60", size = .2) +
  geom_density() +
  xlim(0, 20)+
  labs(x = "First Period Grade", 
        y = "Density",
        title = "Distribution of G1")

p2 <- ggplot(df_MNAR, aes(x = G2, y = ..density..)) +
  geom_histogram(fill = "cornsilk", colour = "grey60", size = .2) +
  geom_density() +
  xlim(0, 20)+
  labs(x = "Second Period Grade", 
        y = "Density",
        title = "Distribution of G2")

p3 <- ggplot(df_MNAR, aes(x = G3, y = ..density..)) +
  geom_histogram(fill = "cornsilk", colour = "grey60", size = .2) +
  geom_density() +
  xlim(0, 20)+
  labs(x = "Final Grade", 
        y = "Density",
        title = "Distribution of G3")

grid.arrange(p1, p2, p3, nrow = 1)
```
## Paired T-test to Compare Grades at Different Periods
Each student takes exams at three different periods within the school year. We are interested in whether the mean score at each period is equal to each other.
In order to evaluate whether the score differences are equal to zero, we propose two hypothesis:
- Hypothesis 1: $$H_0: \mu_{G3} = \mu_{G1}$$ vs. $$H_a: \mu_{G3} \neq \mu_{G1}$$
- Hypothesis 2: $$H_0: \mu_{G3} = \mu_{G2}$$ vs. $$H_a: \mu_{G3} \neq \mu_{G2}$$
We perform two paired t-tests to compare means of G1 versus G3 and G2 versus G3. 

To apply the paired t-test to test for differences between paired measurements, the following assumptions need to hold:

- **Assumption 1: Are the two samples paired?**

Yes, since the data have been collected from recording a student's score at different period.

- **Assumption 2: Are the means normally distributed?**

Yes, the sample size is 395, which is substantially greater than 30. Even though the scores don't follow normal distribution according to Shapiro-Wilk normality test, according to Central Limit Theory, means of samples from a population with finite variance approach a normal distribution regardless of the distribution of the population. 

Thus, the paired t-test is a valid test to check the two hypothesis. We first applied paired t-test to the imputed dataset, where 0's in G2 and G3 are imputed using multiple imputation. 

According to the paired t-test result, we reject the hypothesis that $H_0: \mu_{G3} = \mu_{G1}$ (p-value = 0.001)  and $H_0: \mu_{G3} = \mu_{G2}$ (p-value = 4.36e-05). By plotting histograms of the score difference in Figure 4, we see that both $G3-G2$ and $G3-G1$ are centralized around 0. The difference between G3 and G1 have mean of 0.248 with 95% CI of (0.1 0.39) and The difference between G3 and G1 have mean of 0.18 with 95% CI of (0.094 0.265).
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=3.5, fig.align='center'}
df_g1 <- data.frame(df_MNAR$G1,'G1', df_MNAR$id)
colnames(df_g1) <- c("score", "Name", "id")

df_g2 <- data.frame(df_MNAR$G2,'G2', df_MNAR$id)
colnames(df_g2) <- c("score", "Name", "id")

df_g3 <- data.frame(df_MNAR$G3,'G3', df_MNAR$id)
colnames(df_g3) <- c("score", "Name", "id")

df2 <- rbind(df_g1, df_g2, df_g3)

# Compute t-test
res1 <- t.test(df_MNAR$G3, df_MNAR$G1, paired = TRUE)
res1

# Compute t-test
res2 <- t.test(df_MNAR$G3, df_MNAR$G2, paired = TRUE)
res2
```

```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=3.5, fig.align='center', fig.cap = "Distribution of G3-G1 and G3-G2 (MNAR dataset)."}
p1 <- ggboxplot(df2, 
          x = "Name", 
          y = "score", 
          color = "Name", 
          ylab = "Period", 
          xlab = "Scores")

p2 <- ggplot(df_MNAR, aes(x = G3-G1, y = ..density..)) +
  geom_histogram(fill = "cornsilk", colour = "grey60", size = .2) +
  geom_density() +
  labs(x = "G3-G1", 
        y = "Density")

p3 <- ggplot(df_MNAR, aes(x = G3-G2, y = ..density..)) +
  geom_histogram(fill = "cornsilk", colour = "grey60", size = .2) +
  geom_density() +
  labs(x = "G3-G2", 
        y = "Density")

grid.arrange(p1, p2, p3, nrow = 1)
```
We have also performed paired t-test with the orginal dataset. Where those who didn't attend exams have G2 and G3 as 0's. 

Consistent with the result from MNAR dataset, we reject the hypothesis that $H_0: \mu_{G3} = \mu_{G1}$ (p-value < 2.2e-16)  and $H_0: \mu_{G3} = \mu_{G2}$ (p-value = 0.002994). However, opposite to a slight increment in final grade from G2 and G1 in MNAR dataset, final grade decreases both from G1 ($G3-G1$ Mean (95% CI): -0.49 (-0.76, -0.22)) and from G2 ($G3-G2$ Mean (95% CI): -0.3 (-0.49, -0.10)). The decrement is caused by missing values in G3 and G2. 

Also, by plotting the histogram of score difference in Figure 5, we can see that both $G3-G2$ and $G3-G1$ are left skewed where some students' final grades decreased substantially from first and second periods. 
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=3.5, fig.align='center'}
# Compute t-test
res1 <- t.test(df$G3, df$G1, paired = TRUE)
res1

# Compute t-test
res2 <- t.test(df$G3, df$G2, paired = TRUE)
res2
```
We have also performed paired t-test to dataset where we introduced random missingness to MNAR dataset (the dataset without any missingness). The paired t-test result are very close to that with MNAR dataset.

**Conclusion**

Above all, when the missing values are complete random. Then missing data won't introduce much bias in hypothesis testing we performed. However, if we impute all missing data using 0 as in the original data for missing not due to complete randomness, the test statistics are pretty off from the 'true' statistics. Thus, when data is not missing completely due to randomness, it is necessary to perform data imputation in order to make accurate conclusion. 

```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=3.5, fig.align='center'}
print(paste0("There are ",sum(is.na(mcar$G1)), " missing values in G1 (mcar)"))
print(paste0("There are ",sum(is.na(mcar$G2)), " missing values in G2 (mcar)"))
print(paste0("There are ",sum(is.na(mcar$G3)), " missing values in G3 (mcar)"))

# Compute t-test
res1 <- t.test(mcar$G3, mcar$G1, paired = TRUE)
res1

# Compute t-test
res2 <- t.test(mcar$G3, mcar$G2, paired = TRUE)
res2
```

```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=3.5, fig.align='center', fig.cap = "Distribution of G3-G1 and G3-G2 (original dataset)."}
p2 <- ggplot(df, aes(x = G3-G1, y = ..density..)) +
  geom_histogram(fill = "cornsilk", colour = "grey60", size = .2) +
  geom_density() +
  labs(x = "G3-G1", 
        y = "Density")

p3 <- ggplot(df, aes(x = G3-G2, y = ..density..)) +
  geom_histogram(fill = "cornsilk", colour = "grey60", size = .2) +
  geom_density() +
  labs(x = "G3-G2", 
        y = "Density")

grid.arrange(p2, p3, nrow = 1)
```


## Multiple Linear Regression

### Data Correlation
Data Correlation is a way to understand the relationship between multiple variables and attributes. Using correlation, we can obtain insights whether one or multiple attributes are associated with each other. Figure 6 shows the correlation plot with correlation coefficients for siginificantly correlated variables. The correlation coefficients shown are calculated using Spearman's rank-order correlation method since most of the continuous variables are ordinal.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=8,fig.height=8, fig.align='center', fig.cap="Correlation plot for all numerical features"}
temp <- colnames(df_MNAR[!grepl('factor|logical|character',sapply(df_MNAR,class))])

vars <- temp[1:12]
# correlation plot of features
df_num <-  df_MNAR[,vars]

M<-cor(df_num,  method = "spearman")
testRes = cor.mtest(df_num, conf.level = 0.95, method = "spearman")

corrplot(M, p.mat = testRes$p, 
         method = 'circle', 
         type = 'lower', 
         insig='blank',
         addCoef.col ='black', number.cex = 0.8,  diag=FALSE)


```
**Interpretation of the correlation plot**
According to the correlation plot, G3 are highly correlated with G1 and G2. Number of past failures, age, alcohol consumption, and frequency going out with friends are negatively correlated with final grades. Alcohol consumption is statistically correlated with almost all other variables. Specifically, alcohol consumption is positively correlated with age, free time, past failures and frequency of hang out while negatively correlated with family relationship.

### Mutiple Linear Regression
In this section, we are going to focus on using multiple linear regression to estimate final grades. There are 32 predictors available in our dataset. Since G1 and G2 are highly correlated with G3, we exclude them from our model and to find out how other predictors correlate with G3. The question we are going to answer is that whether students will perform differently in their final exam if they have different number of past failures.

We first build a linear regression with all 30 predictors using imputed MNAR dataset.Figure 7 shows residual plot and QQ plots of the fitting.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=4,fig.height=3.5, fig.align='center'}
df_lr <- df_MNAR[,c(1:30, 33)]

#Baseline Model with all features
lmfit <- lm(G3 ~ ., data=df_lr)

summary(lmfit)
```
**Interpretation of summary table and plot**

Ajusted R square for the linear regression model is 0.26. Note that the R-square value is very low because we didn't include G1 and G2 in our model. According to te linear regression result, gender, study time, past failures, family support, school support, hang out with friends as well as health condition are features significantly correlated with final grades. Specifically, male performs better than female. The more study time a student spend, the higher final grade they will obtain. Past failures, family support, school support, hang out with friends as well as health condition are all negatively correlated with final grade.

We also presented residual plot and Q-Q plot in Figure 7 to check whether the data satisfy the  homoscedasticity and normality assumptions of linear regression. Residual graphs allow use to check if your data shows  homoscedastic, which is when residuals are equal across all values of your predicted variable. According to Figure 7, we see that the red line is pretty flat with small fluctuation and it is very close to the dashed line, which indicates that the data satisfy the homoscedasticity of linear regression.Also according to the Q-Q plot, our data form an approximate straight line and is approximately close to the dashed line (identity line), which indicates that the data meets the normality hypothesis. 
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=6,fig.height=6, fig.align='center', fig.cap="Multiple linear regression results."}
par(mfrow=c(2,2))
plot(lmfit)
```


### Stepwise feature selection
One potential issue of the mutiple linear regression we built is that we have included a lot features that are not correlated with the target variable G3. We devide to the use stepwise regression to perform feature selection to find out the best model.

Stepwise regression can serve as a hypothesis generation tool, giving an indication of how many variables are likely to be useful, and identifying variables as strong candidates for predictive models. Also it involves adding or removing potential explanatory variables and testing for statistical significance after each iteration. The main goal of stepwise regression is to build the best model that gives us the predictive variable which has the largest variance in the outcome variable (ajusted r² or Cp or BIC). 

Figure 8 shows the relationship of Cp, BIC and Ajusted R^2 with number of features included based upon stepwise feature selection. Bayesian information criterion (BIC) is a criterion for model selection in a finite set of models. A lower BIC indicates a lower penalty term and is therefore a better model. R squared computes the scatter of data points around the regression line. It is also known as the determination coefficient, or multiple determination coefficient of multiple regression. The larger R squared, the better the regression model fits the data. According to the plot, we will choose lowest BIC and highest R-squared as our model. We choose to include 10 variables in our final model.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=6,fig.height=6, fig.align='center', fig.cap="Ajudsted R square, Cp and BIC with number of variables according to forward feature selection."}

#Stepwise Regression
M <- regsubsets(G3~(.), data=df_lr,
                 nbest = 1, nvmax=30, 
                 method = 'forward', 
                 intercept = TRUE )

reg.summary <- summary(M)

par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
# which.max(reg.summary$adjr2)
points(which.max(reg.summary$adjr2),reg.summary$adjr2[which.max(reg.summary$adjr2)], col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
# which.min(reg.summary$cp )
points(which.min(reg.summary$cp),reg.summary$cp[which.min(reg.summary$cp)],col="red",cex=2,pch=20)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
# which.min(reg.summary$bic )
points(which.min(reg.summary$bic),reg.summary$bic[which.min(reg.summary$bic)],col="red",cex=2,pch=20)

```

The final model we built has 10 predictors. The model has an adjusted R square of 0.2622, which is slightly higher than our baseline model. Among all features, past failures shows highest correlation with final grade, that the more past failures, the lower final grade the student will have. We also noticed that school support and family support actually won't able to help a student to achieve a high grade. 

We have also built a linear regression model using the original data where missing scores are represented as 0's (due to limitation of report length, we didn't include the results here). The linear regression results show that past failure is significantly negatively correlated with final grades.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=6,fig.height=6, fig.align='center', fig.cap="Ajudsted R square, Cp and BIC with number of variables according to forward feature selection."}
a <- coef(M, 10)
print("Following variables are selected: ")
print(unlist(names(a)))


#final model
final <- lm(I(G3) ~ (age + famsize + `Mother Education` +
                       `Mother Job` + `Father Job` + studytime
                     + `Past Failures` + schoolsup + famsup + goout), data=df_lr)
summary(final)
```
## Logistic Regression to Predict Sign of $G3 - G1$.
In addition to a student's performance in final grade, we are also interested in which factors are related a improvement in scores from first period to final exam the most. 160 students has a higher final grade as comparing to G1 while 235 of them has a lower or similar score.

### Feature Selection
In this section, we perform logistic regression to predict sign of $G3 - G1$. The sign of $G3-G1$ is represented as a indicator variable where its value is 1 if $G3-G1>0$. RFE (recursive feature elimination) is then applied to all features except for G2 and G3 for feature selection before modeling.

RFE applies a backward selection process to find the optimal combination of features. First, it builds a model based on all features and calculates the importance of each feature in the model. Then, it rank-orders the features and removes the one(s) with the least importance iteratively based on accuracy. Feature importance can be computed based on random forest importance criterion.
```{r,echo=FALSE,include=TRUE}
df_MNAR$diff1 <- df_MNAR$G3 - df_MNAR$G1
df_MNAR$ind[df_MNAR$diff1>0]=1
df_MNAR$ind[df_MNAR$diff1<=0]=0
df_MNAR$ind[df_MNAR$diff1<=0]=0
df_MNAR$ind <- as.factor(df_MNAR$ind)


df_MNAR$id <- NULL
df_MNAR$diff1 <- NULL
df_MNAR$diff2 <- NULL
df_MNAR$G3 <- NULL
df_MNAR$G2 <- NULL


print(paste0(sum(df_MNAR$ind==1), " students have a higher score in G3 as comparing to G1"))
print(paste0(sum(df_MNAR$ind==0), " students have a lower or similar score in G3 as comparing to G1"))
```

```{r,echo=FALSE,include=TRUE}
control <- rfeControl(functions = rfFuncs, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 3, # number of repeats
                      number = 5) # number of folds


# Run RFE
result_rfe1 <- rfe(x = df_MNAR[,c(1:31)], 
                   y = df_MNAR[,32], 
                   sizes = c(5:30),
                   rfeControl = control)

# Print the selected features
print("Following variables are selected according to RFE:")
predictors(result_rfe1)
```

The output indicates that RFE recommends 20 features to be included in the model. Accuracy reach the maximum level when 20 features are retained in the model as shown in Figure 9. According to the feature importance generated from RFE model, absence, G1, school and alcohol correlate with improvement in score the most. We include all 20 features recommended and build a logistic regression model to find out their relationship.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=5,fig.height=6, fig.align='center', fig.cap="Accuracy of the model based on the number of features and feature importance."}
g1 <- ggplot(data = result_rfe1, metric = "Accuracy") + theme_bw()

varimp_data <- data.frame(feature = row.names(varImp(result_rfe1))[1:20],
                          importance = varImp(result_rfe1)[1:20, 1])

g2 <- ggplot(data = varimp_data, aes(x = reorder(feature, -importance), y = importance, fill = feature)) +
  geom_bar(stat="identity") + 
  labs(x = "Features", y = "Variable Importance") + 
  theme_bw() + 
  theme(legend.position = "none") +
  rotate_x_text(angle = 45)

grid.arrange(g1, g2, nrow = 2)
```

### Final Logistic Regression Model
Logistic regression shows that absences and G1 correlate with improvement in score the most. Specifically, more absence from school, higher G1 and being in a relationship will lead to a lower score in final grade as compared to first period. Thus, in order to achieve a higher score in his/her final grade, a student should try to ensure as few absense from class as possible.
```{r,echo=FALSE,include=TRUE,message = FALSE, warning = FALSE, out.width='\\textwidth', fig.width=4,fig.height=3.5, fig.align='center', fig.cap="Feature importanec according to RFE."}
simdat <- df_MNAR[c(predictors(result_rfe1), "ind")]

logit <- glm(ind ~  ., data=simdat, family="binomial")
summary(logit)
```




## Conclusions
Much insight was gained about this dataset and the effects that play on a student's performance in a class. It was found that the G3 grade is significantly different from the students' mothers and fathers education along with the mothers job. Being able to partake in extra paid classes, a willingness to persue more classes, and being in a romantic relationship are also different. Through the creation of boxplots, it was found that parents with a higher education degree and students are are wanting a higher degree for themselves will result in a better G3 grade. This is in contrast to students that are in a romantic relationship which sees a lower G3 grade.

Table 2 was able to show that the average G3 grade is lower than the G1 and G2 grades. This could imply that as the year continues, more factors come into play or that the students begin to feel fatigued. With the paired t-test analysis, it was found that this data set is suitable and fits the assumptions. In addition, the hypothesis of equivalence of mean in G1, G2 and g3 were rejected, therefore there is a significant difference with G3 and from both G1 and G2. 

Paired T-test was performed on all three datasets: original data where missing values in scores were represented with 0's, imputed dataset based using multiple imputation and MCAR dataset (where missing values were introduced randomly to imputed dataset) where missing records were dropped from analysis. Imputed dataset and MCAR dataset yield close t-test results while t-test base upon original dataset is a little off from the other two. Thus, accurately dealing with missing values is very important in real world data analysis. Only if missing values are complete random, we can't directly drop it or impute with 0's. 


## References

[1] Paulo Cortez and Alice Silva ((2014)), 'Using data mining to predict secondary school student performance'

[2] Cortez, Paulo. (2014). Student Performance. UCI Machine Learning Repository.

[3] https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html

[4] https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html

[5] https://towardsdatascience.com/effective-feature-selection-recursive-feature-elimination-using-r-148ff998e4f7 

[6] http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/#:~:text=The%20stepwise%20regression%20(or%20stepwise,model%20that%20lowers%20prediction%20error.


